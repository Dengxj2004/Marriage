{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640a234-aeda-47d1-9d31-3b0f14dd9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ORCID 女性姓名变化检测程序\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Manager, Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import signal\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import gender_guesser.detector as gender\n",
    "import Levenshtein\n",
    "from datetime import datetime\n",
    "\n",
    "# 全局配置\n",
    "CONFIG = {\n",
    "    'base_path': '/hy-tmp/ORCID_2024_10_summaries',\n",
    "    'output_file': '/hy-tmp/female_name_changes.csv',\n",
    "    'log_file': '/hy-tmp/orcid_processing.log',\n",
    "    'max_workers': 20,  # 保留2个核心给系统\n",
    "    'chunk_size': 1000,  # 每个进程处理的文件数量\n",
    "    'batch_size': 100,  # 批量写入CSV的记录数\n",
    "}\n",
    "\n",
    "class ORCIDProcessor:\n",
    "    def __init__(self, base_path: str, output_file: str, max_workers: int = 38):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.output_file = output_file\n",
    "        self.max_workers = max_workers\n",
    "        self.gender_detector = gender.Detector()\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # 统计信息\n",
    "        self.stats = {\n",
    "            'total_files': 0,\n",
    "            'processed_files': 0,\n",
    "            'female_records': 0,\n",
    "            'name_changes': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"配置日志系统\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(CONFIG['log_file']),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def extract_name_info(self, soup: BeautifulSoup) -> Optional[Dict]:\n",
    "        \"\"\"从BeautifulSoup对象中提取姓名信息和性别\"\"\"\n",
    "        try:\n",
    "            person_name_tag = soup.find(\"person:name\")\n",
    "            if not person_name_tag:\n",
    "                return None\n",
    "\n",
    "            # 提取person:name基本信息\n",
    "            person_created_date_tag = person_name_tag.find(\"common:created-date\")\n",
    "            given_names_tag = person_name_tag.find(\"personal-details:given-names\")\n",
    "            family_name_tag = person_name_tag.find(\"personal-details:family-name\")\n",
    "\n",
    "            person_created_date = person_created_date_tag.string if person_created_date_tag else None\n",
    "            given_name = given_names_tag.string if given_names_tag else None\n",
    "            family_name = family_name_tag.string if family_name_tag else None\n",
    "\n",
    "            # 性别识别 - 只处理女性\n",
    "            if not given_name:\n",
    "                return None\n",
    "            \n",
    "            gender_result = self.gender_detector.get_gender(given_name.split()[0])\n",
    "            if gender_result not in ['female', 'mostly_female']:\n",
    "                return None  # 只处理女性\n",
    "\n",
    "            # 构建基准姓名 (given_name + family_name)\n",
    "            if not family_name:\n",
    "                return None\n",
    "            base_name = f\"{given_name} {family_name}\".strip()\n",
    "\n",
    "            # 提取other-name信息 - 可能有多个\n",
    "            other_names_data = []\n",
    "            other_name_tags = soup.find_all(\"other-name:other-name\")\n",
    "            \n",
    "            for other_name_tag in other_name_tags:\n",
    "                other_created_date_tag = other_name_tag.find(\"common:created-date\")\n",
    "                other_name_content_tag = other_name_tag.find(\"other-name:content\")\n",
    "                \n",
    "                other_created_date = other_created_date_tag.string if other_created_date_tag else None\n",
    "                other_name_content = other_name_content_tag.string if other_name_content_tag else None\n",
    "                \n",
    "                if other_created_date and other_name_content:\n",
    "                    other_names_data.append({\n",
    "                        'other_name_content': other_name_content,\n",
    "                        'other_created_date': other_created_date\n",
    "                    })\n",
    "\n",
    "            if not other_names_data:\n",
    "                return None\n",
    "\n",
    "            return {\n",
    "                'person_created_date': person_created_date,\n",
    "                'given_name': given_name,\n",
    "                'family_name': family_name,\n",
    "                'base_name': base_name,\n",
    "                'other_names_data': other_names_data,\n",
    "                'gender': gender_result\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Error extracting name info: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def process_single_file(self, file_path: Path, port_name: str) -> Optional[Dict]:\n",
    "        \"\"\"处理单个XML文件\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                soup = BeautifulSoup(file, 'xml')\n",
    "                name_info = self.extract_name_info(soup)\n",
    "\n",
    "                if not name_info:\n",
    "                    return None\n",
    "\n",
    "                is_change, change_data = self.is_name_change_candidate(name_info)\n",
    "                if is_change and change_data:\n",
    "                    path_tag = soup.find(\"common:path\")\n",
    "                    orcid_id = path_tag.string if path_tag else \"unknown\"\n",
    "                    \n",
    "                    return {\n",
    "                        'port_name': port_name,\n",
    "                        'id': orcid_id,\n",
    "                        'person_name': change_data[0],\n",
    "                        'person_date': change_data[1],\n",
    "                        'other_name': change_data[2],\n",
    "                        'other_date': change_data[3],\n",
    "                        'gender': name_info['gender']\n",
    "                    }\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "def _extract_name_parts(full_name: str) -> Dict[str, str]:\n",
    "    \"\"\"从全名中提取姓、名、中间名\"\"\"\n",
    "    parts = full_name.strip().split()\n",
    "    if not parts:\n",
    "        return {'given': '', 'middle': '', 'family': ''}\n",
    "    \n",
    "    given = parts[0]\n",
    "    if len(parts) == 1:\n",
    "        return {'given': given, 'middle': '', 'family': ''}\n",
    "    \n",
    "    family = parts[-1]\n",
    "    middle = ' '.join(parts[1:-1])\n",
    "    return {'given': given, 'middle': middle, 'family': family}\n",
    "\n",
    "\n",
    "def is_name_change_candidate(name_info: Dict) -> Tuple[bool, Optional[List]]:\n",
    "    \"\"\"\n",
    "    判断是否为姓名变化候选者\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- 1. 数据提取 ---\n",
    "        base_name = name_info['base_name']\n",
    "        person_created_date = name_info['person_created_date']\n",
    "        other_names_data = name_info['other_names_data']\n",
    "        family_name = name_info['family_name']\n",
    "        given_name = name_info['given_name']\n",
    "\n",
    "        if not all([base_name, person_created_date, other_names_data, family_name, given_name]):\n",
    "            return False, None\n",
    "\n",
    "        # 解析person:name的创建日期 \n",
    "        try:\n",
    "            person_date = datetime.fromisoformat(person_created_date.replace('Z', '+00:00'))\n",
    "        except ValueError: # 捕获更具体的异常\n",
    "            return False, None\n",
    "\n",
    "        # --- 2. 遍历 other-names，进行筛选和判断 ---\n",
    "        for other_data in other_names_data:\n",
    "            other_name_content = other_data.get('other_name_content')\n",
    "            other_created_date = other_data.get('other_created_date')\n",
    "            \n",
    "            if not other_name_content or not other_created_date:\n",
    "                continue\n",
    "            \n",
    "            # 关联性验证-无需此条件 注释\n",
    "            #if not (str(family_name).lower() in str(other_name_content).lower() or str(given_name).lower() in str(other_name_content).lower()):\n",
    "                #continue\n",
    "\n",
    "            try:\n",
    "                other_date = datetime.fromisoformat(other_created_date.replace('Z', '+00:00'))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            # 时间差过滤\n",
    "            if abs((person_date - other_date).days) < 30:\n",
    "                continue\n",
    "\n",
    "            # --- 3. 姓名标准化与结构化 ---\n",
    "            cleaned_base = ' '.join(str(base_name).lower().strip().replace('.', '').replace(',', '').replace('-', ' ').split())\n",
    "            cleaned_other = ' '.join(str(other_name_content).lower().strip().replace('.', '').replace(',', '').replace('-', ' ').split())\n",
    "            \n",
    "            # Levenshtein 距离\n",
    "            if Levenshtein.distance(cleaned_base, cleaned_other) < 2:\n",
    "                continue\n",
    "\n",
    "            base_parts = _extract_name_parts(cleaned_base)\n",
    "            other_parts = _extract_name_parts(cleaned_other)\n",
    "\n",
    "            if not all([base_parts['given'], other_parts['given'], base_parts['family'], other_parts['family']]):\n",
    "                continue\n",
    "\n",
    "            # --- 4. 核心逻辑判断 ---\n",
    "            \n",
    "            # a) 验证是否为同一个人 (名字部分容错判断)\n",
    "            if Levenshtein.distance(base_parts['given'], other_parts['given']) >= 2:\n",
    "                continue\n",
    "            \n",
    "            # b) 获取并比较完整的姓氏部分\n",
    "            base_full_surname = f\"{base_parts['middle']} {base_parts['family']}\".strip()\n",
    "            other_full_surname = f\"{other_parts['middle']} {other_parts['family']}\".strip()\n",
    "\n",
    "            if Levenshtein.distance(base_full_surname, other_full_surname) < 2:\n",
    "                continue\n",
    "\n",
    "            # c) 排除只增加/减少中间名的情况\n",
    "            is_subset_change = (base_full_surname in other_full_surname or other_full_surname in base_full_surname)\n",
    "            if is_subset_change and Levenshtein.distance(base_parts['family'], other_parts['family']) <= 1:\n",
    "                continue\n",
    "\n",
    "            # d) 排除缩写情况\n",
    "            if (len(base_parts['family']) == 1 and len(other_parts['family']) > 1 and base_parts['family'] == other_parts['family'][0]) or \\\n",
    "               (len(other_parts['family']) == 1 and len(base_parts['family']) > 1 and other_parts['family'] == base_parts['family'][0]):\n",
    "                continue\n",
    "\n",
    "            # --- 5. 确认是候选者 ---\n",
    "            \n",
    "            if person_date < other_date:\n",
    "                older_name, newer_name = cleaned_base, cleaned_other\n",
    "                older_date_str, newer_date_str = person_created_date, other_created_date\n",
    "            else:\n",
    "                older_name, newer_name = cleaned_other, cleaned_base\n",
    "                older_date_str, newer_date_str = other_created_date, person_created_date\n",
    "\n",
    "            return True, [older_name, older_date_str, newer_name, newer_date_str]\n",
    "\n",
    "        return False, None\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, None\n",
    "\n",
    "def process_file_chunk(args):\n",
    "    \"\"\"处理文件块的工作函数\"\"\"\n",
    "    file_paths, port_name, process_id = args\n",
    "    processor = ORCIDProcessor(CONFIG['base_path'], CONFIG['output_file'])\n",
    "    results = []\n",
    "    processed = 0\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            result = processor.process_single_file(file_path, port_name)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "            processed += 1\n",
    "            \n",
    "            # 每处理1000000个文件报告一次进度\n",
    "            if processed % 1000000 == 0:\n",
    "                print(f\"Process {process_id}: Processed {processed}/{len(file_paths)} files in {port_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in process {process_id} processing {file_path}: {e}\")\n",
    "    \n",
    "    return results, processed, len(results)\n",
    "\n",
    "def get_file_chunks(base_path: str, chunk_size: int = 1000) -> List[Tuple]:\n",
    "    \"\"\"获取文件分块信息\"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    chunks = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for portfolio_folder in base_path.iterdir():\n",
    "        if not portfolio_folder.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # 获取文件夹中的所有XML文件\n",
    "        xml_files = list(portfolio_folder.glob(\"*.xml\"))\n",
    "        if not xml_files:\n",
    "            xml_files = list(portfolio_folder.iterdir())  # 如果没有.xml扩展名\n",
    "            \n",
    "        # 将文件分块\n",
    "        for i in range(0, len(xml_files), chunk_size):\n",
    "            file_chunk = xml_files[i:i + chunk_size]\n",
    "            chunks.append((file_chunk, portfolio_folder.name, chunk_id))\n",
    "            chunk_id += 1\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def write_results_batch(results_batch: List[Dict], output_file: str, write_header: bool = False):\n",
    "    \"\"\"批量写入结果到CSV文件\"\"\"\n",
    "    mode = 'w' if write_header else 'a'\n",
    "    with open(output_file, mode, newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['port_name', 'id', 'person_name', 'person_date', \n",
    "                                             'other_name', 'other_date', 'gender'])\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(results_batch)\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"信号处理器\"\"\"\n",
    "    print(\"\\n收到中断信号，正在安全退出...\")\n",
    "    sys.exit(0)\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 注册信号处理器\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    signal.signal(signal.SIGTERM, signal_handler)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 获取文件分块\n",
    "    print(\"正在分析文件结构...\")\n",
    "    chunks = get_file_chunks(CONFIG['base_path'], CONFIG['chunk_size'])\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    print(f\"发现 {total_chunks} 个文件块\")\n",
    "    if total_chunks == 0:\n",
    "        print(\"没有找到需要处理的文件\")\n",
    "        return\n",
    "    \n",
    "    # 初始化输出文件\n",
    "    write_results_batch([], CONFIG['output_file'], write_header=True)\n",
    "    \n",
    "    # 统计变量\n",
    "    total_processed = 0\n",
    "    total_results = 0\n",
    "    results_buffer = []\n",
    "    \n",
    "    try:\n",
    "        # 使用进程池处理\n",
    "        with ProcessPoolExecutor(max_workers=CONFIG['max_workers']) as executor:\n",
    "            print(f\"启动 {CONFIG['max_workers']} 个工作进程...\")\n",
    "            \n",
    "            # 提交所有任务\n",
    "            future_to_chunk = {executor.submit(process_file_chunk, chunk): i \n",
    "                              for i, chunk in enumerate(chunks)}\n",
    "            \n",
    "            # 处理完成的任务\n",
    "            for future in as_completed(future_to_chunk):\n",
    "                chunk_idx = future_to_chunk[future]\n",
    "                try:\n",
    "                    chunk_results, chunk_processed, chunk_found = future.result()\n",
    "                    \n",
    "                    total_processed += chunk_processed\n",
    "                    total_results += chunk_found\n",
    "                    results_buffer.extend(chunk_results)\n",
    "                    \n",
    "                    # 批量写入结果\n",
    "                    if len(results_buffer) >= CONFIG['batch_size']:\n",
    "                        write_results_batch(results_buffer, CONFIG['output_file'])\n",
    "                        results_buffer = []\n",
    "                    \n",
    "                    # 进度报告\n",
    "                    progress = (chunk_idx + 1) / total_chunks * 100\n",
    "                    print(f\"进度: {progress:.1f}% ({chunk_idx + 1}/{total_chunks}), \"\n",
    "                          f\"已处理: {total_processed:,}, 发现变化: {total_results:,}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"处理块 {chunk_idx} 时出错: {e}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n用户中断程序执行\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"程序执行出错: {e}\")\n",
    "        return\n",
    "    finally:\n",
    "        # 写入剩余结果\n",
    "        if results_buffer:\n",
    "            write_results_batch(results_buffer, CONFIG['output_file'])\n",
    "    \n",
    "    # 最终统计\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"处理完成!\")\n",
    "    print(f\"总处理文件数: {total_processed:,}\")\n",
    "    print(f\"发现姓名变化: {total_results:,}\")\n",
    "    print(f\"处理时间: {duration/3600:.1f} 小时 ({duration/60:.1f} 分钟)\")\n",
    "    print(f\"平均速度: {total_processed/duration:.0f} 文件/秒\")\n",
    "    print(f\"结果已保存至: {CONFIG['output_file']}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":   \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
